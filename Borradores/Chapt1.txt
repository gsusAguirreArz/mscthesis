LHC potential problems to address: the nature of mass, the dimensionality of space, the unification of the fundamental forces, the particle nature of antimatter and the fine tuning of the standard model.
Within HEP particles physicists for several years they have used Multivariate Analysis (MVA) however outside of physics these techniques would be considered examples of machine learning.
Techniques used: ANN, kernel density estimation, svm, genetic algo, random forests, boosted decision trees. Failed when the dimensionality of the dara grew large.
The emergence of deep learning in 2012 outperformed the previous state of the art. Handle high dimensionality and more complex problems.
High dimensional data.
At LHC Collisions occur with a frequency of ~ 40MHz.
Each collision produces a large number of particles. The LHC detectors hace O(10^8) sensors to record these particles.
Observations are fundamentally probabilistic.
Low level tasks: particles identification | reconstruction of the particle energy and momentum
High level tasks: searches for new particles energy and momentum
Statistical model p(x|theta) describes the probability of observing x given the params of a theory theta.
The high dimensionality  and large volume of lhc data pose a problem because the statistical model p(x|theta) over the high dimensional space of their experimental data is not known explicitly in terms of an equation that can be evaluated. 
Instead we have access to large samples of simulated data generated by stochastic simulation programs that model the physics of particle interactions.
If the data were fairly low dimensional (d<5) the problem of estimating the unknown statistical from the simulated samples would not be difficult. In a single dimension, N samples may be required to describe the source pdf. In d dimensions, the number of samples required grows to the power of the datas dimensionality: O(N^d)  if d>5 | d>10 requires impractical or impossible computational resources, regardless of the speed of the sample generator.

Simulation Tools such as PYTHIA, HERWING, MAD-GRAPH, SHERPA, GEANT, CORSIKA are referred to as Monte Carlo tools.
We can think of a simulated data ser {x_i}_i=1^N as being N iid samples from p(x|theta), where theta corresponds to the setting of the simulator. EG. generate samples of interactions involving a Higgs Boson for any desired mass value.
Goal of simulation: approximate p(x|theta) by sampling from an enormous space of unobserved, or latent, processes: p(x|theta)= int p(x,z|theta)dz.
A fixed value of z specifies everything about the simulated events, from the momentum of the initial particles created in the hard scattering to the detailed interactions in the detector. Usually called z as monte carlo truth.
Most reconstruction algos can be regarded as estimates of some components of z (particle type, momentum, energy, etc.) given the observed data x.
Simulation fulfills a second experimental need: in addition to an estimate of p(x|theta) the simulation provides a dataset {x_i, z_i}_i=1^N.(1)

Fast simulation
Fast simulation is valuable because the full simulators, which faithfully describe the low level interactions of particles with matter, are very computationally intensive and consume a significant fraction of the computing budgets of current experimental collaborations.
State of the art physics  base simulations like Geant4 provide an excellent description of hadronic interactions with detector material over a wide range in energy and particle type. However these models are slow.
A great interest in using GAN to speed up simulations and maybe on day tune directly on collision or testbeam data.
Both GAN and VAE are promising methods for accurate generation in High Dimensional feature spaces,
The biggest challenge for these methods is to quantify their performance (their performance is not well summarized by the value of the loss function).
Proposals:
Jet level fast simulations based on gans [ https://arxiv.org/abs/1903.02433 ][https://link.springer.com/article/10.1007/s41781-018-0008-x ] (2)
Simulation of the electromagnetic showers in a multi layer calorimeter (on the most computationally expensive steps for a low level simulator.) [https://arxiv.org/abs/1705.02355 ]
Simulating jet images [https://arxiv.org/abs/1701.05927 ](1)
Generative models are excellent at interpolation but are likely unreliable for extrapolation.(2)
Future simulation tools built on GANs may provide important speed boosts for the slower elements of the simulation chain, or they may be sophisticated enough to provide end to end simulations.
Viewed as a non parametric fast simulation.
Strategic planning for HL-LHC software efforts [https://arxiv.org/abs/1712.06592 ][https://arxiv.org/abs/1712.06982 ]
Alternative approaches to fast simulation[ . Larochelle H, Murray I. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, p. 29. Cambridge, MA: MIT Press (2011) ] [Papamakarios G, Murray I, Pavlakou T. In Proceedings of the 31st Conference on Neural Information Processing Systems (NIPS17), p. 2335. New York: Curran][https://arxiv.org/abs/1505.05770 ][https://arxiv.org/abs/1410.8516 ][https://arxiv.org/abs/1506.02169 ][https://arxiv.org/abs/1606.04934 ](1)

Concerns
Interpretability and Reliance on simulation
Progress on the computational side is not always matched by gains in physical understanding and is heavily reliant on simulation programs.
Non non-parametric nature of NN makes it difficult to interpret the solution. ( not easily inspected to discover the structure of its learned solution)
Trade off between performance and interpretability.
Use of ML also complicates the scientific communication and theoretical interpretation of LHC results. Difficult to glean the essential physics, it is also an impediment to reinterpreting the result in the context of a different theoretical model.
Network learned solution relies on a well modeled physical effect or an overlooked weak point.
In other cases the weak points of the modeling are well known to physicists who would prefer a learned solution that avoids detailed reliance on these features.
EG most simulated samples of collisions that result in jets use either PYTHIA or HERWIG to model the parton shower, but these two heuristic approaches can make significantly different predictions about the jet images.
One way to address this issue is to explicitly parametrize the network in the space of the unknown nuisance parameter, so that the dependance can be studied or constrained in data.[https://arxiv.org/abs/1506.02169][https://link.springer.com/article/10.1140/epjc/s10052-016-4099-4 ]
Other way is to explicitly reduce the depende of the network on aspects that are sensitive to the underlying uncertainties, making the resulting network less sensitive to these uncertainties.[https://arxiv.org/abs/1611.01046 ](1)

Computing need[https://twiki.cern.ch/twiki/bin/view/CMSPublic/CMSOfflineComputingResults ][https://arxiv.org/abs/2009.04509 ][https://www.researchgate.net/publication/342769710_CMS_Software_and_Offline_preparation_for_future_runs ]

References:

(1)https://arxiv.org/abs/1806.11484
(2)https://arxiv.org/abs/1709.04464 
Online references:
Code:
https://github.com/iml-wg/HEP-ML-Resources
https://github.com/onnx/onnx
http://pyro.ai/
http://edwardlib.org/
https://zenodo.org/record/4310003


Papers:
https://www.researchgate.net/publication/326110883_Deep_Learning_and_its_Application_to_LHC_Physics
https://iml-wg.github.io/HEPML-LivingReview/
https://arxiv.org/abs/1610.09787 
